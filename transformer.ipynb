{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "import torch\n",
    "#from utils import *\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sarc_responses(train_file, test_file, comment_file, lower=True):\n",
    "  '''loads SARC data from csv files\n",
    "  Args:\n",
    "    train_file: csv file with train sequences\n",
    "    test_file: csv file with train sequences\n",
    "    comment_file: json file with details about all comments\n",
    "    lower: boolean; if True, converts comments to lowercase\n",
    "  Returns:\n",
    "    train_sequences, train_labels, test_sequences, test_labels\n",
    "    train_sequences: {'ancestors': list of ancestors for all sequences,\n",
    "                      'responses': list of responses for all sequences}\n",
    "    train_labels: list of labels for responses for all sequences.\n",
    "  '''\n",
    "\n",
    "  with open(comment_file, 'r') as f:\n",
    "    comments = json.load(f)\n",
    "\n",
    "  train_docs = {'ancestors': [], 'responses': []}\n",
    "  train_labels = []\n",
    "  with open(train_file, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='|')\n",
    "    for row in reader:\n",
    "      ancestors = row[0].split(' ')\n",
    "      responses = row[1].split(' ')\n",
    "      labels = row[2].split(' ')\n",
    "      if lower:\n",
    "        train_docs['ancestors'].append([comments[r]['text'].lower() for r in ancestors])\n",
    "        train_docs['responses'].append([comments[r]['text'].lower() for r in responses])\n",
    "      else:\n",
    "        train_docs['ancestors'].append([comments[r]['text'] for r in ancestors])\n",
    "        train_docs['responses'].append([comments[r]['text'] for r in responses])\n",
    "      train_labels.append(labels)\n",
    "\n",
    "  test_docs = {'ancestors': [], 'responses': []}\n",
    "  test_labels = []\n",
    "  with open(test_file, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='|')\n",
    "    for row in reader:\n",
    "      ancestors = row[0].split(' ')\n",
    "      responses = row[1].split(' ')\n",
    "      labels = row[2].split(' ')\n",
    "      if lower:\n",
    "        test_docs['ancestors'].append([comments[r]['text'].lower() for r in ancestors])\n",
    "        test_docs['responses'].append([comments[r]['text'].lower() for r in responses])\n",
    "      else:\n",
    "        test_docs['ancestors'].append([comments[r]['text'] for r in ancestors])\n",
    "        test_docs['responses'].append([comments[r]['text'] for r in responses])\n",
    "      test_labels.append(labels)\n",
    "\n",
    "  return train_docs, test_docs, train_labels, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SARC = '/home/halbared/SS23/NLP/main/'\n",
    "train_file = SARC+'train-balanced.csv'\n",
    "test_file = SARC+'test-balanced.csv'\n",
    "comment_file = SARC+'comments.json'\n",
    "\n",
    "  # Load SARC pol/main sequences with labels.\n",
    "print('Load SARC data')\n",
    "train_seqs, test_seqs, train_labels, test_labels =load_sarc_responses(train_file, test_file, comment_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_seqs)\n",
    "train_seqs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_resp = train_seqs['responses']\n",
    "test_resp = test_seqs['responses']\n",
    "\n",
    "  # Split into first and second responses and their labels.\n",
    "  # {0: list_of_first_responses, 1: list_of_second_responses}\n",
    "train_docs = {i: [l[i] for l in train_resp] for i in range(2)}\n",
    "test_docs = {i: [l[i] for l in test_resp] for i in range(2)}\n",
    "train_labels = {i: [2*int(l[i])-1 for l in train_labels] for i in range(2)}\n",
    "test_labels = {i: [2*int(l[i])-1 for l in test_labels] for i in range(2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_docs_tok = train_docs[0] + train_docs[1]\n",
    "test_all_docs_tok = test_docs[0] + test_docs[1]\n",
    "train_all_labels = np.array(train_labels[0] + train_labels[1])\n",
    "test_all_labels = np.array(test_labels[0] + test_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {'comment': train_all_docs_tok,\n",
    "        'label': train_all_labels}\n",
    "  \n",
    "# Create DataFrame\n",
    "df_train = pd.DataFrame(train_data_dict)\n",
    "\n",
    "test_data_dict = {'comment': test_all_docs_tok,\n",
    "        'label': test_all_labels}\n",
    "  \n",
    "# Create DataFrame\n",
    "df_test = pd.DataFrame(test_data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()\n",
    "df_test.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" df_train_balanced_url='/home/halbared/SS23/NLP/archive/train-balanced-sarcasm.csv'\n",
    "df_train_balanced=pd.read_csv(df_train_balanced_url)\n",
    "df_test_balanced_url='/home/halbared/SS23/NLP/archive/test-balanced.csv'\n",
    "df_test_balanced=pd.read_csv(df_test_balanced_url) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "        print(df.columns)\n",
    "        self.labels = [label for label in df['label']]\n",
    "        \n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['comment']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx],np.array(self.labels[idx])\n",
    "\n",
    "        \"\"\" batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx) \"\"\"\n",
    "\n",
    "        #return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "                  \n",
    "EPOCHS = 2\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, df_train, df_test, LR, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
