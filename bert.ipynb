{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from accelerate) (1.12.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.6.0->accelerate) (4.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy requests nlpaug\n",
    "#!pip install tensformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nlpaug.augmenter.word as nlpaw\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig,get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "# Note: this notebook requires torch >= 1.10.0\n",
    "torch.__version__\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "# !pip install tensorboard\n",
    "!pip install accelerate -U\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "TBOARD_LOGS = os.path.join(os.getcwd(), \"tboard_logs\", \"test\")\n",
    "if not os.path.exists(TBOARD_LOGS):\n",
    "    os.makedirs(TBOARD_LOGS)\n",
    "\n",
    "shutil.rmtree(TBOARD_LOGS)\n",
    "writer = SummaryWriter(TBOARD_LOGS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall pillow\n",
    "# !pip install pillow==9.4.0\n",
    "\n",
    "\n",
    "df=pd.read_csv('../train-balanced-sarcasm.csv')\n",
    "labels=df[\"label\"].to_list()\n",
    "comments=df[\"comment\"].astype(str).to_list()\n",
    "df2=pd.DataFrame({\"comments\":comments,\"labels\":labels})\n",
    "df2_short={\"text\":comments,\"labels\":labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test_valid, y_train, y_test_valid = train_test_split(comments, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "x_test, x_valid, y_test, y_valid = train_test_split(x_test_valid, y_test_valid, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2, output_attentions = False,\n",
    "    output_hidden_states = False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 5e-5,\n",
    "                              eps = 1e-08\n",
    "                              )\n",
    "\n",
    "# Run on GPU\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(texts):\n",
    "#     tokenized_texts=[]\n",
    "#     for text in texts:\n",
    "#         if type(text)!='str':\n",
    "#             # print(type(text))\n",
    "#             text=str(text)\n",
    "#         token=tokenizer(text,\n",
    "#         truncation=True,\n",
    "#         padding=True,\n",
    "#         max_length=128)\n",
    "#         tokenized_texts.append(token)\n",
    "\n",
    "#     return tokenized_texts    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sent):\n",
    "    return  tokenizer.encode_plus(\n",
    "                        sent,\n",
    "                        add_special_tokens = True,\n",
    "                        padding=True,\n",
    "                        max_length = 256,\n",
    "                        truncation=True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,data,labels):\n",
    "      self.data=data\n",
    "      self.labels=labels   \n",
    "    def __len__(self):\n",
    "        return len(self.data)    \n",
    "    def __getitem__(self, idx):\n",
    "       print(type(self.data[idx]))\n",
    "       return preprocess_function(self.data[idx]) ,torch.tensor(self.labels[idx])\n",
    "       return [preprocess_function(d) for d in self.data[idx]],torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset=CustomDataset(x_train,labels=y_train)\n",
    "test_dataset=CustomDataset(x_test,labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_dataloader=torch.utils.data.DataLoader(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([[[  101, 27327,  2271,  3504, 10140,   102]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1]]])},\n",
       " tensor([0])]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(test_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, epoch, device):\n",
    "    \"\"\" Training a model for one epoch \"\"\"\n",
    "    \n",
    "    loss_list = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "         \n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "         \n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "         \n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "         \n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    mean_loss = np.mean(loss_list)\n",
    "    return mean_loss, loss_list\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, eval_loader, criterion, device):\n",
    "    \"\"\" Evaluating the model for either validation or test \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_list = []\n",
    "    \n",
    "    for images, labels in eval_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass only to get logits/output\n",
    "        outputs = model(images)\n",
    "                 \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "            \n",
    "        # Get predictions from the maximum value\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct += len( torch.where(preds==labels)[0] )\n",
    "        total += len(labels)\n",
    "                 \n",
    "    # Total correct predictions and loss\n",
    "    accuracy = correct / total * 100\n",
    "    loss = np.mean(loss_list)\n",
    "    \n",
    "    return accuracy, loss\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, criterion, train_loader, valid_loader, num_epochs, tboard=None, start_epoch=0):\n",
    "    \"\"\" Training a model for a given number of epochs\"\"\"\n",
    "    \n",
    "    train_loss = []\n",
    "    val_loss =  []\n",
    "    loss_iters = []\n",
    "    valid_acc = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "           \n",
    "        # validation epoch\n",
    "        model.eval()  # important for dropout and batch norms\n",
    "        accuracy, loss = eval_model(\n",
    "                    model=model, eval_loader=valid_loader,\n",
    "                    criterion=criterion, device=device\n",
    "            )\n",
    "        valid_acc.append(accuracy)\n",
    "        val_loss.append(loss)\n",
    "        writer.add_scalar(f'Accuracy/Valid', accuracy, global_step=epoch+start_epoch)\n",
    "        writer.add_scalar(f'Loss/Valid', loss, global_step=epoch+start_epoch)\n",
    "        \n",
    "        # training epoch\n",
    "        model.train()  # important for dropout and batch norms\n",
    "        mean_loss, cur_loss_iters = train_epoch(\n",
    "                model=model, train_loader=train_loader, optimizer=optimizer,\n",
    "                criterion=criterion, epoch=epoch, device=device\n",
    "            )\n",
    "        scheduler.step()\n",
    "        train_loss.append(mean_loss)\n",
    "        writer.add_scalar(f'Loss/Train', mean_loss, global_step=epoch+start_epoch)\n",
    "\n",
    "        loss_iters = loss_iters + cur_loss_iters\n",
    "        \n",
    "        if(epoch % 5 == 0 or epoch==num_epochs-1):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"    Train loss: {round(mean_loss, 5)}\")\n",
    "            print(f\"    Valid loss: {round(loss, 5)}\")\n",
    "            print(f\"    Accuracy: {accuracy}%\")\n",
    "            print(\"\\n\")\n",
    "    \n",
    "    print(f\"Training completed\")\n",
    "    return train_loss, val_loss, loss_iters, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (2.13.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (1.24.3)\n",
      "Requirement already satisfied: dill in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (2.29.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (0.15.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (23.1)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hussein ahmed\\miniconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mg:\\German\\Master\\Sommer23\\NLP\\NLP-project-team26\\bert.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmy_awesome_model\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m2e-5\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     save_strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     load_best_model_at_end\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     push_to_hub\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/g%3A/German/Master/Sommer23/NLP/NLP-project-team26/bert.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32m<string>:111\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, no_cuda, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, xpu_backend)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\HUSSEIN AHMED\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\training_args.py:1340\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(version\u001b[39m.\u001b[39mparse(torch\u001b[39m.\u001b[39m__version__)\u001b[39m.\u001b[39mbase_version) \u001b[39m==\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m2.0.0\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16:\n\u001b[0;32m   1335\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1337\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1339\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[1;32m-> 1340\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1341\u001b[0m     \u001b[39mand\u001b[39;00m (get_xla_device_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGPU\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1342\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1343\u001b[0m ):\n\u001b[0;32m   1344\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1345\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1346\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m (`--fp16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1347\u001b[0m     )\n\u001b[0;32m   1349\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1350\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframework \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1351\u001b[0m     \u001b[39mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16 \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1357\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\HUSSEIN AHMED\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\training_args.py:1764\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1760\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1761\u001b[0m \u001b[39mThe device used by this process.\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1763\u001b[0m requires_backends(\u001b[39mself\u001b[39m, [\u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m-> 1764\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_devices\n",
      "File \u001b[1;32mc:\\Users\\HUSSEIN AHMED\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\utils\\generic.py:54\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, obj, objtype)\u001b[0m\n\u001b[0;32m     52\u001b[0m cached \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, attr, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     53\u001b[0m \u001b[39mif\u001b[39;00m cached \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     cached \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget(obj)\n\u001b[0;32m     55\u001b[0m     \u001b[39msetattr\u001b[39m(obj, attr, cached)\n\u001b[0;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m cached\n",
      "File \u001b[1;32mc:\\Users\\HUSSEIN AHMED\\miniconda3\\envs\\myenv\\lib\\site-packages\\transformers\\training_args.py:1672\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[0;32m   1671\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_accelerate_available(min_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m0.20.1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1672\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m   1673\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1674\u001b[0m         )\n\u001b[0;32m   1675\u001b[0m     AcceleratorState\u001b[39m.\u001b[39m_reset_state(reset_partial_state\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1676\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistributed_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
