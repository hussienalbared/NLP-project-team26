{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy requests nlpaug\n",
    "#!pip install tensformers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nlpaug.augmenter.word as nlpaw\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig,get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "# Note: this notebook requires torch >= 1.10.0\n",
    "torch.__version__\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import AutoTokenizer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "# !pip install tensorboard\n",
    "# !pip install accelerate -U\n",
    "# !pip uninstall pillow\n",
    "# !pip install pillow==9.4.0\n",
    "# !pip install evaluate\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df=pd.read_csv('../train-balanced-sarcasm.csv')\n",
    "labels=df[\"label\"].to_list()\n",
    "comments=df[\"comment\"].astype(str).to_list()\n",
    "df2=pd.DataFrame({\"comments\":comments,\"labels\":labels})\n",
    "df2_short={\"text\":comments,\"labels\":labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train, x_test_valid, y_train, y_test_valid = train_test_split(comments, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "x_test, x_valid, y_test, y_valid = train_test_split(x_test_valid, y_test_valid, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# # import BERT-base pretrained model\n",
    "# bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Load the BERT tokenizer\n",
    "# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(sent):\n",
    "    \n",
    "    return tokenizer (\n",
    "                        sent,\n",
    "                        # add_special_tokens = True,\n",
    "                        # padding=True,\n",
    "                        # max_length = 256,\n",
    "                        truncation=True,\n",
    "                        # return_attention_mask = True,\n",
    "                        # return_tensors = 'pt',\n",
    "                        \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,encodings,labels):\n",
    "      self.encodings=encodings\n",
    "      self.labels=labels   \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    #    return preprocess_function(self.data[idx]) ,torch.tensor(self.labels[idx])\n",
    "\n",
    "train_encodings = tokenizer(x_train, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(x_valid, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(x_test, truncation=True, padding=True)\n",
    "\n",
    "\n",
    "train_dataset=CustomDataset(train_encodings,labels=y_train)\n",
    "test_dataset=CustomDataset(val_encodings,labels=y_test)\n",
    "train_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_dataloader=torch.utils.data.DataLoader(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_seq_len = 128\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_train = tokenizer.batch_encode_plus(\n",
    "#     x_train,\n",
    "#     max_length = max_seq_len,\n",
    "#     pad_to_max_length=True,\n",
    "#     truncation=True,\n",
    "#     return_token_type_ids=False\n",
    "# )\n",
    "\n",
    "# # tokenize and encode sequences in the validation set\n",
    "# tokens_val = tokenizer.batch_encode_plus(\n",
    "#     x_valid,\n",
    "#     max_length = max_seq_len,\n",
    "#     pad_to_max_length=True,\n",
    "#     truncation=True,\n",
    "#     return_token_type_ids=False\n",
    "# )\n",
    "\n",
    "# # tokenize and encode sequences in the test set\n",
    "# tokens_test = tokenizer.batch_encode_plus(\n",
    "#     x_test,\n",
    "#     max_length = max_seq_len,\n",
    "#     pad_to_max_length=True,\n",
    "#     truncation=True,\n",
    "#     return_token_type_ids=False\n",
    "# )\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for train set\n",
    "# train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "# train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "# train_y = torch.tensor(y_train)\n",
    "\n",
    "# # for validation set\n",
    "# val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "# val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "# val_y = torch.tensor(y_valid)\n",
    "\n",
    "# # for test set\n",
    "# test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "# test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "# test_y = torch.tensor(y_test)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# #define a batch size\n",
    "# batch_size = 32\n",
    "\n",
    "# # wrap tensors\n",
    "# train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# # sampler for sampling the data during training\n",
    "# train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# # dataLoader for train set\n",
    "# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# # wrap tensors\n",
    "# val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# # sampler for sampling the data during training\n",
    "# val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# # dataLoader for validation set\n",
    "# val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a,b=next(iter(train_dataloader))\n",
    "# type(a),a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # freeze all the parameters\n",
    "# for param in bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERT_Arch(nn.Module):\n",
    "\n",
    "#     def __init__(self, bert):\n",
    "      \n",
    "#       super(BERT_Arch, self).__init__()\n",
    "\n",
    "#       self.bert = bert \n",
    "      \n",
    "#       # dropout layer\n",
    "#       self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "#       # relu activation function\n",
    "#       self.relu =  nn.ReLU()\n",
    "\n",
    "#       # dense layer 1\n",
    "#       self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "#       # dense layer 2 (Output layer)\n",
    "#       self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "#       #softmax activation function\n",
    "#       self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "#     #define the forward pass\n",
    "#     def forward(self, sent_id, mask):\n",
    "\n",
    "#       #pass the inputs to the model  \n",
    "#       _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      \n",
    "#       x = self.fc1(cls_hs)\n",
    "\n",
    "#       x = self.relu(x)\n",
    "\n",
    "#       x = self.dropout(x)\n",
    "\n",
    "#       # output layer\n",
    "#       x = self.fc2(x)\n",
    "      \n",
    "#       # apply softmax activation\n",
    "#       x = self.softmax(x)\n",
    "\n",
    "#       return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BERT_Arch(bert)\n",
    "\n",
    "# push the model to GPU\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # optimizer from hugging face transformers\n",
    "# from transformers import AdamW\n",
    "\n",
    "# # define the optimizer\n",
    "# optimizer = AdamW(model.parameters(), lr = 1e-3)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# #compute the class weights\n",
    "# class_wts = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "\n",
    "# print(class_wts)\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # convert class weights to tensor\n",
    "# weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "# weights = weights.to(device)\n",
    "\n",
    "# # loss function\n",
    "# cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# # number of training epochs\n",
    "# epochs = 10\n",
    "     \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
