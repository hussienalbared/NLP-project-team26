{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy requests nlpaug\n",
    "# !pip install tensformers\n",
    "# !pip install evaluate\n",
    "# !pip install tensorboard\n",
    "# !pip install accelerate -U\n",
    "# !pip uninstall pillow\n",
    "# !pip install pillow==9.4.0\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from transformers import  DataCollatorWithPadding\n",
    "from datasets import  load_metric\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer,BertForSequenceClassification\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "from transformers import get_linear_schedule_with_warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df=pd.read_csv('../train-balanced-sarcasm.csv')\n",
    "labels=df[\"label\"].to_list()\n",
    "comments=df[\"comment\"].astype(str).to_list()\n",
    "df2=pd.DataFrame({\"comments\":comments,\"labels\":labels})\n",
    "df2_short={\"text\":comments,\"labels\":labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x_train, x_test_valid, y_train, y_test_valid = train_test_split(comments, labels, test_size=0.33, random_state=42)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(comments, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# x_test, x_valid, y_test, y_valid = train_test_split(x_test_valid, y_test_valid, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels = 2, \n",
    ")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\n",
    "\n",
    "#     \"distilbert-base-uncased\", num_labels=2\n",
    "\n",
    "# )\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#     num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "#                     # You can increase this for multi-class tasks.   \n",
    "#     output_attentions = False, # Whether the model returns attentions weights.\n",
    "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "# )\n",
    "model.to(device)\n",
    "     \n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (962 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  9827\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "\n",
    "# For every sentence...\n",
    "for sent in x_train:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_time(elapsed):\n",
    "   \n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,encodings,labels):\n",
    "      self.encodings=encodings\n",
    "      self.labels=labels   \n",
    "    def __len__(self):\n",
    "        return len(self.encodings)    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    " \n",
    "max_length = 256                     \n",
    "train_encodings = tokenizer(x_train, truncation=True, padding=True,   add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_length, \n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt')\n",
    "val_encodings = tokenizer(x_valid, truncation=True, padding=True,   add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_length,           \n",
    "                        return_attention_mask = True,   \n",
    "                        return_tensors = 'pt',)\n",
    "\n",
    "\n",
    "train_dataset=CustomDataset(train_encodings,labels=y_train)\n",
    "eval_dataset=CustomDataset(val_encodings,labels=y_valid)\n",
    "\n",
    "train_dataloader=torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "eval_dataloader=torch.utils.data.DataLoader(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dataloader))\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    print(eval_pred)\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "epochs = 6\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.34\n",
      "  Training epcoh took: 0:00:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.00\n",
      "  Validation Loss: 0.75\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 2 / 6 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_521170/2475671310.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 0.32\n",
      "  Training epcoh took: 0:00:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.00\n",
      "  Validation Loss: 0.75\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 3 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.35\n",
      "  Training epcoh took: 0:00:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.00\n",
      "  Validation Loss: 0.75\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 4 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.36\n",
      "  Training epcoh took: 0:00:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.00\n",
      "  Validation Loss: 0.75\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 5 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.31\n",
      "  Training epcoh took: 0:00:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.00\n",
      "  Validation Loss: 0.75\n",
      "  Validation took: 0:00:00\n",
      "======== Epoch 6 / 6 ========\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.28\n",
      "  Training epcoh took: 0:00:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.00\n",
      "  Validation Loss: 0.75\n",
      "  Validation took: 0:00:00\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:00:01 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # print(vars(batch))\n",
    "\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "      \n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "\n",
    "        \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "       \n",
    "        output = model(b_input_ids,\n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # print(vars(output))\n",
    "        loss=output.loss\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    \n",
    "        loss.backward()\n",
    " \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in eval_dataloader:\n",
    "        b_input_ids = batch['input_ids'].to(device)\n",
    "        b_input_mask = batch['attention_mask'].to(device)\n",
    "        b_labels = batch['labels'].to(device)\n",
    "        \n",
    "   \n",
    "        with torch.no_grad():        \n",
    "            # (loss, logits) = model(b_input_ids,\n",
    "            #                        attention_mask=b_input_mask,\n",
    "            #                        labels=b_labels)\n",
    "            output = model(b_input_ids,\n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "\n",
    "        # print(vars(output))\n",
    "        loss=output.loss    \n",
    "        logits=output.logits\n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(eval_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(eval_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"my_model\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=1,\n",
    "#     per_device_eval_batch_size=1,\n",
    "#     num_train_epochs=10,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     push_to_hub=False,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=eval_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "#     # data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "    \n",
    "# )\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
